Improving our lock Mechanism:
------------------------------

The purpose of this program is to overcome the problem we faced in last example,
where the lock was not able to provide mutual exclusion between multiple
threads due to lack of atomicity of the 'lock value fetch' and 'update' in the
'my_lock' function.

------------------------------------------------------------------------------
Note: This lock mechanism improvement code has been written with 4 threads to
test the lock better. Hence, please run this example on a device with 4 or more
cores. Less number of cores also works fine, but the lock contention is lesser.
------------------------------------------------------------------------------

Below was the code to acquire lock in previous example,

/*
 * Return only when you get the lock, otherwise keep waiting for the lock.
 */
void my_lock(int *p, char *str)
{
    /* Loop, until the lock is not free. */
    while(*p) {
        printf("%s : Waiting for the lock\n", str);
    }

    printf("%s : Acquired the lock\n", (char *) str);
    *p = 1;
}

The problem with this function is that, the fetching of the value of lock, which
is 'while(*p)' statement and the update of the value of lock, which is
'*p = 1;', is not atomic, and OS switches between the threads between these two
operations of a thread.

If we can make these 2 operations atomic, we can solve the above issue.

To solve this issue in our 'spin lock', we can use the method of 'Test-and-set'.

Test-and-set : The test-and-set instruction is an instruction used to write
(set) 1 to a memory location and return its old value as a single atomic
(i.e., non-interruptible) operation.

Below is our 'my_lock' function re-written to make the test-and-set atomic.

void my_lock(int *p)
{
    int val = 1;

    /* Loop, until the lock is not free. */
    do {
        __asm__ volatile("xchg %0, %1" : "+q" (val), "+m" (*p));
    } while(val);

    return;
}

Here, the logic is simple. We are just exchanging the value of the '*p' and
'val' in an atomic operation.

Lets assume that when first thread starts to execute this, it just gets the
current value at memory of 'p' and just puts the value 1 to it.
Then, in the loop it checks whether that fetched value from *p was 0 or 1.

If its still 1, it has to again go in the loop.
If its 0, that means earlier the lock had the value 0(FREE), but now, has been
made to 1(ACQUIRED) and now the lock is with this thread. Hence, it returns.

When other threads try to take this lock with the same method 'my_lock', they
get stuck in the while loop until this thread frees the lock by calling
'my_unlock'.

[ ] gcc lock.c -lpthread
[ ] ./a.out
Final value of counter : 40000000


Spin-lock performance: For a system with single processor, spin-lock can be
---------------------  very waste-full in terms of program speed.

Assume that there are four threads running on a single processor and all of them
run some critical section locked with a common lock.

Now, assume that one thread(T1) has acquired the lock and executing the critical
section code, at this same time, OS switches to the other thread(T2) and now
thread T2 tries to take the same lock and wastes some CPU time in the loop in
order to get the lock, but since the lock has already been taken by the T1, T2
keeps waiting, and after some time OS again switches to another thread T3. Here
also, its the same thing as T2, it keeps waiting for some to acquire lock, and
then OS switches to T4. Here also, the same thing as T2 and T3, because the lock
is held by T1, which is inside its critical section.

This wastes a lot of CPU cycle because all the threads are running on a single
core. When threads wait for the lock, T1 can not execute the critical section
code to release the lock soon.
Means, multiple threads operating with a common lock on a single thread wastes
a lot of CPU cycles.

Now, assume that we have 4 threads and 4 CPUs and each of the thread executes
on separate CPU. In this case, the wait by other threads(T2, T3, T4) and the
execution of the critical section by T1 is happening on different CPUs. Hence,
the execution of T1 is not blocked by T2, T3 and T4 waiting and the program
progress faster compare the single processor.
